# Phase 2 - Plan D√©taill√© de D√©veloppement

**Date**: 7 octobre 2025
**Version**: 2.1.0 (Mise √† jour avec gestion timeouts)
**Dur√©e estim√©e**: 6-7 semaines
**Status**: üìã Planifi√©

**üÜï Nouveaut√©s Version 2.1**:
- ‚úÖ Syst√®me de gestion des timeouts avec rollback automatique
- ‚úÖ Notifications utilisateur temps r√©el (WebSocket + Email)
- ‚úÖ Monitoring pr√©ventif des quotas API (alertes √† 80%)
- ‚úÖ Endpoint retry intelligent avec recommandations
- ‚úÖ Architecture compl√®te document√©e dans [ARCHITECTURE_TIMEOUT_MANAGEMENT.md](ARCHITECTURE_TIMEOUT_MANAGEMENT.md)

---

## üìã Executive Summary

La Phase 2 vise √† **optimiser, compl√©ter et stabiliser** le syst√®me ScorpiusAO sur la base des r√©sultats valid√©s de la Phase 1. Focus sur:

1. **Performance** - R√©duire temps d'ingestion de 6 min √† < 30s
2. **Compl√©tude** - Activer Usage #1 (Knowledge Base)
3. **Qualit√©** - Mesurer et am√©liorer Recall@5, MRR, NDCG
4. **Stabilit√©** - Tests automatis√©s et monitoring
5. **üÜï Robustesse** - Gestion timeouts avec rollback et retry intelligent

### R√©sultats Phase 1 √† Am√©liorer

| M√©trique | Phase 1 (Actuel) | Phase 2 (Objectif) | Gain |
|----------|------------------|-------------------|------|
| **Temps ingestion** | 6 min (92 embeddings) | < 30s | **92%** |
| **Documents ing√©r√©s** | 1/4 (CCTP.pdf seulement) | 4/4 (tous) | **300%** |
| **Confidence Q&A** | 50-65% | 75-85% | **+15-20pts** |
| **Recall@5** | Non mesur√© | > 80% | **Baseline** |
| **Coverage tests** | ~20% | > 80% | **+60pts** |

---

## üéØ Objectifs Strat√©giques Phase 2

### 1. **Performance & Scalabilit√©** (Priorit√© 1)

**Probl√®me identifi√©**:
- Ingestion 92 embeddings prend 6 minutes (92 appels API s√©quentiels)
- Seul CCTP.pdf ing√©r√©, CCAP.pdf et RC.pdf skipped (timeout)

**Objectifs**:
- ‚úÖ Batch processing OpenAI (100 chunks/requ√™te)
- ‚úÖ Ingestion compl√®te de tous les documents
- ‚úÖ Temps total < 2 minutes pour 3 documents

**M√©triques de succ√®s**:
- Temps ingestion: **< 30 secondes** (vs 6 min actuellement)
- Documents ing√©r√©s: **100%** (vs 25% actuellement)
- Co√ªt/tender: **< $0.05** (vs $0.14 actuellement)

---

### 2. **Knowledge Base - Usage #1** (Priorit√© 2)

**Probl√®me identifi√©**:
- Endpoint `/suggestions` impl√©ment√© mais non test√©
- Manque historical_tenders dans la base
- Pas de donn√©es pour valider la recherche KB

**Objectifs**:
- ‚úÖ Ing√©rer 5+ tenders historiques gagnants
- ‚úÖ Tester endpoint `/suggestions` avec donn√©es r√©elles
- ‚úÖ Valider qualit√© des suggestions (pertinence, diversit√©)

**M√©triques de succ√®s**:
- Historical tenders: **‚â• 5 tenders gagnants** ing√©r√©s
- Recall@10 (KB): **> 80%**
- Diversit√© suggestions: **‚â• 3 tenders sources diff√©rents**

---

### 3. **Qualit√© & Tests** (Priorit√© 3)

**Probl√®me identifi√©**:
- Recall@5, MRR, NDCG non mesur√©s
- Pas de jeu de donn√©es test (golden dataset)
- Coverage tests ~20%

**Objectifs**:
- ‚úÖ Cr√©er golden dataset (50 Q&A avec r√©ponses attendues)
- ‚úÖ Mesurer Recall@5, MRR, NDCG sur benchmark
- ‚úÖ Tests unitaires + int√©gration (coverage > 80%)

**M√©triques de succ√®s**:
- Recall@5: **> 80%**
- MRR: **> 0.7**
- Test coverage: **> 80%**
- CI/CD: Tests auto sur PRs

---

### 4. **Parsing Tableaux** (Priorit√© 4)

**Probl√®me identifi√©**:
- Tableaux complexes mal extraits (crit√®res, SLA, p√©nalit√©s)
- 3 solutions document√©es mais non impl√©ment√©es

**Objectifs**:
- ‚úÖ Solution 1 (Enrichissement prompt) - Quick win
- ‚è≥ Solution 2 (Post-processing spatial) - Si temps
- ‚è≥ Solution 3 (Camelot) - Phase 3

**M√©triques de succ√®s**:
- Qualit√© extraction tableaux: **> 85%** (vs ~60% actuellement)
- Crit√®res d'√©valuation d√©tect√©s: **100%**

---

### 5. **üÜï Gestion Timeouts & Retry** (Priorit√© 1 - CRITIQUE)

**Probl√®me identifi√©**:
- Aucune gestion des timeouts API (OpenAI, Claude, PostgreSQL)
- Donn√©es partielles restent en base apr√®s √©chec
- Utilisateur non notifi√© des causes d'√©chec
- Pas de syst√®me de retry intelligent

**Objectifs**:
- ‚úÖ Rollback automatique avec savepoints SQLAlchemy
- ‚úÖ Notifications temps r√©el (WebSocket + Email)
- ‚úÖ Monitoring pr√©ventif quotas (alertes 80%)
- ‚úÖ Endpoint retry avec recommandations actionnables
- ‚úÖ Diagnostics pr√©cis (quota, timeout, erreur r√©seau)

**M√©triques de succ√®s**:
- Rollback: **100%** des donn√©es apr√®s timeout
- Notifications: **< 5 secondes** apr√®s √©chec
- Warnings pr√©ventifs: **Alertes √† 80%** quota
- Retry success rate: **> 90%** apr√®s correction
- Z√©ro corruption donn√©es

**Documentation**: Voir [ARCHITECTURE_TIMEOUT_MANAGEMENT.md](ARCHITECTURE_TIMEOUT_MANAGEMENT.md)

---

## üìÖ Planning D√©taill√© (7 Semaines)

**üÜï Version 2.1**: Ajout de 1 semaine pour gestion timeouts & retry (Sprint 1 √©tendu √† 3 semaines)

### Sprint 1: Performance, Ingestion & Gestion Timeouts (3 semaines)

**üÜï Changement majeur**: Sprint 1 passe de 2 √† 3 semaines pour int√©grer le syst√®me de gestion des timeouts, rollback et retry.

**Semaine 1: Batch Processing OpenAI**

**Objectif**: R√©duire temps ingestion de 6 min √† < 30s

**T√¢ches**:
1. **Refactoriser `RAGService.ingest_document_sync()`** (2 jours)
   - Remplacer appels s√©quentiels par batch processing
   - Grouper chunks par batch de 100 (limite OpenAI: 2048 inputs)
   - G√©rer erreurs par batch avec retry
   ```python
   # Avant: 92 appels √ó ~4s = 6 minutes
   for chunk in chunks:
       embedding = openai.Embedding.create(input=chunk)

   # Apr√®s: 1 appel √ó 10s = 10 secondes
   batch_size = 100
   for i in range(0, len(chunks), batch_size):
       batch = chunks[i:i+batch_size]
       embeddings = openai.Embedding.create(input=batch)
   ```

2. **Impl√©menter connexion pool PostgreSQL** (1 jour)
   - SQLAlchemy pool avec pool_size=10, max_overflow=20
   - R√©duire latence INSERT embeddings (bulk insert)

3. **Tests de performance** (1 jour)
   - Benchmark ingestion sur VSGP-AO (377 sections)
   - Mesurer temps par document (CCTP, CCAP, RC)
   - Valider co√ªt total < $0.05

**Livrables**:
- ‚úÖ Ingestion CCTP.pdf: < 10 secondes
- ‚úÖ Temps total 3 documents: < 30 secondes
- ‚úÖ Tests automatis√©s de performance

**Semaine 2: Ingestion Multi-Documents**

**Objectif**: Ing√©rer 100% des documents (vs 25% actuellement)

**T√¢ches**:
1. **Fixer logique ingestion `process_tender_documents()`** (1 jour)
   - Assurer que STEP 2 traite TOUS les documents
   - Ne pas skip CCAP.pdf, RC.pdf apr√®s CCTP.pdf
   - Logging d√©taill√© par document

2. **Parall√©lisation documents** (2 jours)
   ```python
   from concurrent.futures import ThreadPoolExecutor

   with ThreadPoolExecutor(max_workers=3) as executor:
       futures = {
           executor.submit(ingest_doc, cctp): "CCTP",
           executor.submit(ingest_doc, ccap): "CCAP",
           executor.submit(ingest_doc, rc): "RC"
       }
       for future in as_completed(futures):
           doc_name = futures[future]
           print(f"‚úÖ {doc_name} ing√©r√©")
   ```

3. **Validation E2E compl√®te** (1 jour)
   - R√©-ing√©rer tender VSGP-AO avec nouveau code
   - V√©rifier 350+ embeddings cr√©√©s (vs 92 actuellement)
   - Tester Q&A sur CCAP.pdf et RC.pdf

**Livrables**:
- ‚úÖ 4/4 documents ing√©r√©s (100% vs 25%)
- ‚úÖ 350+ embeddings cr√©√©s
- ‚úÖ Q&A fonctionne sur tous les documents

**üÜï Semaine 3: Gestion Timeouts, Rollback & Retry**

**Objectif**: Assurer robustesse du pipeline avec gestion erreurs et retry intelligent

**T√¢ches**:
1. **Cr√©er exceptions custom** (0.5 jour)
   - Fichier: `backend/app/core/exceptions.py`
   - Classes: `OpenAITimeoutError`, `ClaudeTimeoutError`, `QuotaExceededError`, etc.
   - Attributs: message, error_code, details, recommendations
   - M√©thode `to_dict()` pour s√©rialisation API

2. **Impl√©menter Transaction Service** (1.5 jours)
   - Fichier: `backend/app/services/transaction_service.py`
   - M√©thode `savepoint()` context manager avec SQLAlchemy nested transactions
   - M√©thode `track_progress()` pour observabilit√© pipeline
   - M√©thode `clear_partial_data()` pour cleanup apr√®s timeout
   - Tests rollback sur exceptions simul√©es

3. **Cr√©er Notification Service** (1 jour)
   - Fichier: `backend/app/services/notification_service.py`
   - M√©thode `notify_timeout()` avec WebSocket + Email
   - M√©thode `notify_quota_warning()` pour alertes pr√©ventives
   - M√©thode `notify_retry_success()` apr√®s retry r√©ussi
   - Templates email avec diagnostics et recommandations

4. **Impl√©menter Quota Monitor** (1.5 jours)
   - Fichier: `backend/app/services/quota_monitor.py`
   - M√©thode `track_api_call()` avec Redis
   - M√©thode `check_quota()` avant op√©rations co√ªteuses
   - M√©thode `get_usage_report()` pour dashboard
   - Alertes automatiques √† 80% usage

5. **Cr√©er Endpoint Retry** (0.5 jour)
   - Fichier: `backend/app/api/v1/endpoints/retry.py`
   - POST `/tenders/{id}/retry` avec pre-checks
   - Analyse erreur pr√©c√©dente + recommandations
   - Validation quotas avant relance
   - Historique tentatives

6. **Int√©grer au Pipeline Celery** (1 jour)
   - Wrapper `process_tender_documents()` avec savepoints
   - Ajout try/except avec gestion exceptions custom
   - Logs d√©taill√©s progr√®s (STEP X/6)
   - Tests E2E timeout ‚Üí rollback ‚Üí notification ‚Üí retry

**Livrables**:
- ‚úÖ Rollback automatique fonctionne (100% cas)
- ‚úÖ Notifications < 5s apr√®s √©chec
- ‚úÖ Endpoint retry avec diagnostics
- ‚úÖ Tests coverage > 80% pour nouveau code
- ‚úÖ Documentation [ARCHITECTURE_TIMEOUT_MANAGEMENT.md](ARCHITECTURE_TIMEOUT_MANAGEMENT.md)

---

### Sprint 2: Knowledge Base - Usage #1 (2 semaines)

**Semaine 4: Ingestion Historical Tenders**

**Objectif**: Cr√©er la base de connaissances avec 5+ tenders gagnants

**T√¢ches**:
1. **Pr√©parer 5 tenders historiques** (1 jour)
   - S√©lectionner 5 tenders gagn√©s (secteur public IT)
   - Collecter documents: CCTP, CCAP, M√©mo Technique
   - Anonymiser donn√©es sensibles (montants, dates)

2. **Script ingestion KB** (2 jours)
   ```bash
   python scripts/ingest_historical_tender.py \
     --tender_id=<uuid> \
     --status=won \
     --documents=CCTP.pdf,CCAP.pdf,MEMO.pdf \
     --metadata='{"score_obtained": 85.2, "rank": 1}'
   ```

   - Cr√©er entr√©e `historical_tenders` avec m√©tadonn√©es
   - Ing√©rer embeddings avec `document_type="historical_tender"`
   - Lier √† `past_proposals` si m√©mo technique fourni

3. **Validation donn√©es KB** (1 jour)
   - V√©rifier 5 tenders dans `historical_tenders`
   - Compter embeddings par tender (~300 chacun)
   - Tester recherche vectorielle sur KB

**Livrables**:
- ‚úÖ 5 historical_tenders en base
- ‚úÖ ~1500 embeddings KB cr√©√©s
- ‚úÖ Script CLI r√©utilisable

**Semaine 5: Endpoint Suggestions & Tests**

**Objectif**: Valider la qualit√© des suggestions KB

**T√¢ches**:
1. **Tests endpoint `/suggestions`** (2 jours)
   ```bash
   curl -X POST "/tenders/{id}/suggestions" -d '{
     "section_type": "memo_technique",
     "requirements": [
       "Gestion des changements selon ITIL",
       "CMDB et gestion des configurations",
       "Support niveau 1 et 2"
     ],
     "top_k": 10
   }'
   ```

   - Tester avec 10 requ√™tes types (m√©thodologie, processus, SLA)
   - V√©rifier groupement par tender source
   - Valider filtrage `status="won"`

2. **Mesurer qualit√© suggestions** (2 jours)
   - **Recall@10**: % r√©ponses pertinentes dans top-10
   - **Diversit√©**: Nombre de tenders sources distincts
   - **Pertinence**: Score similarit√© moyen

   Cr√©er jeu de test:
   ```python
   test_cases = [
       {
           "query": "Processus ITIL requis",
           "expected_sections": ["4.1.5", "7.1"],  # Sections attendues
           "min_similarity": 0.7
       },
       # ... 20 test cases
   ]
   ```

3. **Documentation usage KB** (1 jour)
   - Guide utilisateur: Comment formuler requ√™tes
   - Exemples de suggestions obtenues
   - Best practices (nombre de requirements, formulation)

**Livrables**:
- ‚úÖ Endpoint `/suggestions` valid√©
- ‚úÖ Recall@10: > 80%
- ‚úÖ Documentation utilisateur KB

---

### Sprint 3: Qualit√© & Tests Avanc√©s (2 semaines)

**Semaine 6: Golden Dataset & M√©triques**

**Objectif**: Mesurer objectivement la qualit√© du syst√®me Q&A

**T√¢ches**:
1. **Cr√©er golden dataset** (2 jours)
   - 50 questions sur VSGP-AO avec r√©ponses attendues
   - 10 questions par cat√©gorie:
     - Processus techniques (ITIL, normes)
     - Crit√®res d'√©valuation
     - P√©nalit√©s et risques
     - D√©lais et planning
     - Exclusions et conditions

   Format:
   ```json
   {
     "question": "Quels sont les processus ITIL couverts ?",
     "expected_answer": {
       "sections": ["4.1.5", "7.1"],
       "keywords": ["Change Management", "CMDB", "Release Management"],
       "min_confidence": 0.6
     },
     "category": "processus_techniques"
   }
   ```

2. **Impl√©menter suite d'√©valuation** (2 jours)
   ```python
   # scripts/evaluate_rag_quality.py
   def evaluate_recall_at_k(dataset, k=5):
       """Mesure % r√©ponses correctes dans top-k"""

   def evaluate_mrr(dataset):
       """Mean Reciprocal Rank: 1/rang premi√®re bonne r√©ponse"""

   def evaluate_ndcg(dataset):
       """Normalized Discounted Cumulative Gain: qualit√© classement"""
   ```

3. **Benchmarking** (1 jour)
   - Ex√©cuter sur 50 questions
   - G√©n√©rer rapport avec:
     - Recall@5, Recall@10
     - MRR
     - NDCG@5, NDCG@10
     - Temps moyen par question
     - Co√ªt par question

**Livrables**:
- ‚úÖ Golden dataset (50 Q&A)
- ‚úÖ Rapport benchmark complet
- ‚úÖ M√©triques baseline √©tablies

**Semaine 7: Tests Unitaires & CI/CD**

**Objectif**: Coverage > 80%, CI/CD automatis√©

**T√¢ches**:
1. **Tests unitaires services** (2 jours)
   - `test_rag_service.py`: chunking, embeddings, recherche
   - `test_parser_service.py`: extraction PDF, structure
   - `test_llm_service.py`: prompts, cache, parsing r√©ponses
   - Target: 80% coverage chaque service

2. **Tests int√©gration** (1 jour)
   - Pipeline E2E complet (upload ‚Üí analyse ‚Üí Q&A)
   - Tests avec mocks OpenAI/Anthropic (pas de co√ªt)
   - Fixtures PostgreSQL avec donn√©es test

3. **CI/CD GitHub Actions** (1 jour)
   ```yaml
   # .github/workflows/tests.yml
   name: Tests
   on: [push, pull_request]
   jobs:
     test:
       runs-on: ubuntu-latest
       services:
         postgres:
           image: pgvector/pgvector:pg15
       steps:
         - uses: actions/checkout@v3
         - name: Run tests
           run: pytest --cov=app --cov-report=xml
         - name: Upload coverage
           uses: codecov/codecov-action@v3
         - name: Fail if coverage < 80%
           run: |
             coverage report --fail-under=80
   ```

**Livrables**:
- ‚úÖ Coverage > 80%
- ‚úÖ CI/CD GitHub Actions
- ‚úÖ Tests automatiques sur PRs

---

## üîß T√¢ches Techniques D√©taill√©es

### Task 1: Batch Processing OpenAI (Priorit√© 1)

**Fichier**: `app/services/rag_service.py`
**M√©thode**: `ingest_document_sync()`
**Effort**: 2 jours

**Avant (S√©quentiel)**:
```python
def ingest_document_sync(self, db, document_id, chunks, document_type, metadata):
    embeddings_created = 0

    for chunk in chunks:
        # 1 appel API par chunk ‚Üí 92 appels = 6 minutes
        embedding_response = self.client.embeddings.create(
            input=chunk["text"],
            model="text-embedding-3-small"
        )
        embedding_vector = embedding_response.data[0].embedding

        # INSERT dans PostgreSQL
        db_embedding = DocumentEmbedding(
            document_id=document_id,
            chunk_text=chunk["text"],
            embedding=embedding_vector,
            document_type=document_type,
            meta_data=metadata
        )
        db.add(db_embedding)
        embeddings_created += 1

    db.commit()
    return embeddings_created
```

**Apr√®s (Batch)**:
```python
def ingest_document_sync(self, db, document_id, chunks, document_type, metadata):
    BATCH_SIZE = 100  # Limite OpenAI: 2048 inputs max
    embeddings_created = 0

    # Grouper chunks par batch
    for i in range(0, len(chunks), BATCH_SIZE):
        batch_chunks = chunks[i:i+BATCH_SIZE]
        batch_texts = [c["text"] for c in batch_chunks]

        try:
            # 1 appel API pour 100 chunks ‚Üí 1 appel vs 100
            embedding_response = self.client.embeddings.create(
                input=batch_texts,  # Liste de textes
                model="text-embedding-3-small"
            )

            # Bulk insert dans PostgreSQL
            db_embeddings = []
            for j, chunk in enumerate(batch_chunks):
                embedding_vector = embedding_response.data[j].embedding

                db_embedding = DocumentEmbedding(
                    document_id=document_id,
                    chunk_text=chunk["text"],
                    embedding=embedding_vector,
                    document_type=document_type,
                    meta_data={**metadata, "chunk_index": i + j}
                )
                db_embeddings.append(db_embedding)

            db.bulk_save_objects(db_embeddings)
            embeddings_created += len(db_embeddings)

        except Exception as e:
            print(f"‚ö†Ô∏è Batch {i//BATCH_SIZE + 1} failed: {e}")
            # Retry batch individuellement en cas d'erreur
            for chunk in batch_chunks:
                # Fallback s√©quentiel pour ce batch
                pass

    db.commit()
    return embeddings_created
```

**Gains**:
- Temps: 6 min ‚Üí 10-15 secondes (**-95%**)
- Appels API: 92 ‚Üí 1 (**-99%**)
- Co√ªt r√©seau: R√©duit latence cumul√©e

---

### Task 2: Parall√©lisation Documents (Priorit√© 1)

**Fichier**: `app/tasks/tender_tasks.py`
**M√©thode**: `process_tender_documents()`
**Effort**: 2 jours

**Avant (S√©quentiel)**:
```python
def process_tender_documents(tender_id: UUID):
    # STEP 2: Cr√©er embeddings pour CHAQUE document s√©quentiellement
    for document in documents:
        print(f"Processing {document.filename}...")

        # Extraction sections
        sections = extract_sections(document)

        # Chunking
        chunks = chunk_sections(sections)

        # Ingestion (6 minutes pour CCTP.pdf)
        embeddings_count = rag_service.ingest_document_sync(
            db, document.id, chunks, "tender", metadata
        )

        # Apr√®s CCTP.pdf (6 min), reste 0 min ‚Üí timeout
        # CCAP.pdf et RC.pdf skipped
```

**Apr√®s (Parall√®le)**:
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_tender_documents(tender_id: UUID):
    # STEP 2: Cr√©er embeddings pour TOUS documents en parall√®le

    def ingest_single_document(document):
        """Ing√©rer un document (thread-safe)"""
        # Cr√©er session DB par thread
        db = SessionLocal()
        try:
            sections = extract_sections(document)
            chunks = chunk_sections(sections)

            count = rag_service.ingest_document_sync(
                db, document.id, chunks, "tender", metadata
            )

            return {
                "filename": document.filename,
                "embeddings": count,
                "status": "success"
            }
        except Exception as e:
            return {
                "filename": document.filename,
                "error": str(e),
                "status": "failed"
            }
        finally:
            db.close()

    # Ex√©cuter 3 documents en parall√®le
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(ingest_single_document, doc): doc.filename
            for doc in documents
        }

        results = []
        for future in as_completed(futures):
            filename = futures[future]
            result = future.result()
            results.append(result)
            print(f"‚úÖ {filename}: {result}")

    # Logs r√©capitulatifs
    total_embeddings = sum(r.get("embeddings", 0) for r in results)
    print(f"üéØ Total: {total_embeddings} embeddings pour {len(documents)} documents")
```

**Gains**:
- Temps: 3 √ó 10s = 30s en parall√®le (vs 18s s√©quentiel th√©orique, mais pratique 6+ min avec timeouts)
- Fiabilit√©: Tous les documents ing√©r√©s
- Observabilit√©: Logs par document

---

### Task 3: Endpoint Suggestions Validation (Priorit√© 2)

**Fichier**: `app/api/v1/endpoints/tenders.py`
**M√©thode**: `get_content_suggestions()`
**Effort**: 2 jours

**Tests √† impl√©menter**:

```python
# tests/api/test_suggestions.py

def test_suggestions_basic(client, historical_tenders_fixture):
    """Test basique endpoint /suggestions"""
    response = client.post(
        f"/api/v1/tenders/{tender_id}/suggestions",
        json={
            "section_type": "memo_technique",
            "requirements": [
                "Gestion des changements selon ITIL",
                "CMDB et gestion des configurations"
            ],
            "top_k": 10
        }
    )

    assert response.status_code == 200
    data = response.json()

    # V√©rifications
    assert data["section_type"] == "memo_technique"
    assert len(data["suggestions"]) > 0
    assert data["total_found"] >= 3
    assert data["source"] == "past_proposals"

    # V√©rifier groupement par tender
    for tender_title, items in data["suggestions"].items():
        assert len(items) > 0
        for item in items:
            assert item["similarity_score"] > 0.5
            assert "content" in item
            assert "historical_tender_id" in item


def test_suggestions_filtering_won_only(client, db_session):
    """V√©rifier que seuls tenders gagn√©s sont retourn√©s"""
    # Cr√©er 2 tenders: 1 gagn√©, 1 perdu
    won_tender = HistoricalTender(status="won", ...)
    lost_tender = HistoricalTender(status="lost", ...)
    db_session.add_all([won_tender, lost_tender])
    db_session.commit()

    response = client.post(f"/api/v1/tenders/{tender_id}/suggestions", json={...})
    data = response.json()

    # V√©rifier que seul won_tender appara√Æt
    historical_ids = [
        item["historical_tender_id"]
        for items in data["suggestions"].values()
        for item in items
    ]
    assert str(won_tender.id) in historical_ids
    assert str(lost_tender.id) not in historical_ids


def test_suggestions_diversity(client):
    """V√©rifier diversit√© des sources"""
    response = client.post(f"/api/v1/tenders/{tender_id}/suggestions", json={
        "requirements": ["Support infrastructure IT"],
        "top_k": 10
    })
    data = response.json()

    # Nombre de tenders sources distincts
    tender_titles = list(data["suggestions"].keys())
    assert len(tender_titles) >= 3, "Au moins 3 tenders sources diff√©rents"


def test_suggestions_recall_at_10(client, golden_dataset):
    """Mesurer Recall@10"""
    correct_retrievals = 0
    total_queries = len(golden_dataset)

    for test_case in golden_dataset:
        response = client.post(f"/api/v1/tenders/{tender_id}/suggestions", json={
            "requirements": test_case["requirements"],
            "top_k": 10
        })
        data = response.json()

        # V√©rifier si sections attendues pr√©sentes dans top-10
        all_sections = [
            item.get("section_number")
            for items in data["suggestions"].values()
            for item in items
        ]

        expected_sections = test_case["expected_sections"]
        if any(sec in all_sections for sec in expected_sections):
            correct_retrievals += 1

    recall_at_10 = correct_retrievals / total_queries
    assert recall_at_10 > 0.8, f"Recall@10 = {recall_at_10:.2%} (objectif: >80%)"
```

---

### Task 4: Golden Dataset Creation (Priorit√© 3)

**Fichier**: `tests/fixtures/golden_dataset.json`
**Effort**: 2 jours

**Structure**:
```json
{
  "dataset_version": "1.0",
  "tender_id": "3cfc8207-f275-4e53-ae0c-bead08cc45b7",
  "tender_title": "VSGP - Infog√©rance Infrastructure IT",
  "created_at": "2025-10-07",
  "questions": [
    {
      "id": "q001",
      "category": "processus_techniques",
      "question": "Quels sont les processus ITIL couverts dans le CCTP ?",
      "expected_answer": {
        "sections": ["4.1.5", "7.1", "4.1.5.9", "4.1.5.10"],
        "keywords": [
          "Change Management",
          "Configuration Management",
          "Release Management",
          "CMDB"
        ],
        "documents": ["CCTP.pdf"],
        "min_confidence": 0.6,
        "min_similarity": 0.65
      },
      "difficulty": "medium",
      "importance": "high"
    },
    {
      "id": "q002",
      "category": "criteres_evaluation",
      "question": "Quels sont les crit√®res d'√©valuation et leurs pond√©rations ?",
      "expected_answer": {
        "sections": ["6.1", "6.2"],
        "keywords": [
          "Prix",
          "Valeur technique",
          "Pond√©ration",
          "Notation"
        ],
        "documents": ["RC.pdf"],
        "min_confidence": 0.5,
        "acceptable_not_found": true  # Info peut √™tre absente
      },
      "difficulty": "hard",
      "importance": "critical"
    },
    {
      "id": "q003",
      "category": "penalites_risques",
      "question": "Quelles sont les p√©nalit√©s financi√®res en cas de non-respect des SLA ?",
      "expected_answer": {
        "sections": ["18.3", "18.1", "18.2"],
        "keywords": [
          "P√©nalit√©s",
          "SLA",
          "Niveau de service",
          "Palier"
        ],
        "documents": ["CCAP.pdf"],
        "min_confidence": 0.65,
        "expected_data_format": "table"  # Donn√©es structur√©es attendues
      },
      "difficulty": "medium",
      "importance": "high"
    }
    // ... 47 autres questions
  ]
}
```

**Cat√©gories (10 questions chacune)**:
1. **processus_techniques**: ITIL, normes ISO, m√©thodologies
2. **criteres_evaluation**: Pond√©rations, notation, bar√®mes
3. **penalites_risques**: P√©nalit√©s, SLA, risques financiers
4. **delais_planning**: Dates limites, jalons, dur√©es
5. **exclusions_conditions**: Motifs exclusion, conditions obligatoires

**M√©thodologie de cr√©ation**:
1. Lire manuellement VSGP-AO (3 documents)
2. Identifier 50 questions pertinentes pour bid manager
3. Noter sections sources et r√©ponses attendues
4. Varier difficult√© (easy 20%, medium 50%, hard 30%)
5. Valider avec expert m√©tier (bid manager)

---

### Task 5: CI/CD Setup (Priorit√© 3)

**Fichiers**: `.github/workflows/*.yml`
**Effort**: 1 jour

**Workflow 1: Tests** (`.github/workflows/tests.yml`):
```yaml
name: Tests & Coverage

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: pgvector/pgvector:pg15
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s

      rabbitmq:
        image: rabbitmq:3-management-alpine
        options: >-
          --health-cmd "rabbitmq-diagnostics ping"
          --health-interval 10s

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio

      - name: Run migrations
        run: |
          cd backend
          alembic upgrade head
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db

      - name: Run tests with coverage
        run: |
          cd backend
          pytest --cov=app --cov-report=xml --cov-report=term
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
          RABBITMQ_URL: amqp://guest:guest@localhost:5672/
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./backend/coverage.xml
          flags: backend

      - name: Fail if coverage < 80%
        run: |
          cd backend
          coverage report --fail-under=80
```

**Workflow 2: Linting** (`.github/workflows/lint.yml`):
```yaml
name: Linting

on: [push, pull_request]

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install linters
        run: |
          pip install ruff black mypy

      - name: Run ruff
        run: |
          cd backend
          ruff check app/ --select=E,F,I

      - name: Run black
        run: |
          cd backend
          black --check app/

      - name: Run mypy
        run: |
          cd backend
          mypy app/ --ignore-missing-imports
```

**Workflow 3: E2E Tests** (`.github/workflows/e2e.yml`):
```yaml
name: E2E Tests

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

jobs:
  e2e:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Start services
        run: |
          docker compose up -d
          sleep 30  # Wait for services

      - name: Run E2E tests
        run: |
          cd backend
          python scripts/tests/run_full_suite.py

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-results
          path: backend/test_results/
```

---

## üìä M√©triques de Suivi Phase 2

### Dashboard Hebdomadaire

| M√©trique | Baseline (Phase 1) | Semaine 1 | Semaine 2 | Semaine 3 | Semaine 4 | Semaine 5 | Semaine 6 | Objectif |
|----------|-------------------|-----------|-----------|-----------|-----------|-----------|-----------|----------|
| **Temps ingestion** | 6 min | | | | | | | < 30s |
| **Documents ing√©r√©s (%)** | 25% | | | | | | | 100% |
| **Embeddings cr√©√©s** | 92 | | | | | | | 350+ |
| **Confidence Q&A moyenne** | 58% | | | | | | | 75%+ |
| **Recall@5** | N/A | | | | | | | > 80% |
| **MRR** | N/A | | | | | | | > 0.7 |
| **Historical tenders** | 0 | | | | | | | 5+ |
| **Test coverage** | ~20% | | | | | | | > 80% |

### KPIs de Succ√®s

**Performance** (Sprint 1):
- ‚úÖ Temps ingestion < 30s pour 3 documents
- ‚úÖ 100% documents ing√©r√©s (vs 25%)
- ‚úÖ Co√ªt ingestion < $0.05/tender

**Fonctionnalit√©** (Sprint 2):
- ‚úÖ 5+ historical_tenders ing√©r√©s
- ‚úÖ Endpoint `/suggestions` op√©rationnel
- ‚úÖ Recall@10 (KB) > 80%

**Qualit√©** (Sprint 3):
- ‚úÖ Golden dataset (50 Q&A) cr√©√©
- ‚úÖ Recall@5 > 80%
- ‚úÖ MRR > 0.7
- ‚úÖ Test coverage > 80%

---

## üí∞ Budget Phase 2

### Co√ªts de D√©veloppement

| Poste | Effort | Co√ªt Unitaire | Total |
|-------|--------|---------------|-------|
| **Dev Backend** | 7 semaines (+1) | $5,000/semaine | $35,000 |
| **üÜï Gestion Timeouts/Retry** | Inclus Sprint 1, Sem 3 | - | (Inclus ci-dessus) |
| **QA/Testing** | 1 semaine | $4,000/semaine | $4,000 |
| **DevOps** | 0.5 semaine | $5,000/semaine | $2,500 |
| **Total Dev** | | | **$41,500** |

**üÜï D√©tail ajout Semaine 3**:
- Exceptions custom + Transaction Service: 2 jours ($1,600)
- Notification Service + Quota Monitor: 2.5 jours ($2,000)
- Endpoint Retry + Integration: 1.5 jours ($1,200)
- Tests gestion timeouts: 1 jour ($800)
- **Sous-total Sem 3**: 7 jours = **$5,600** (inclus dans $35k ci-dessus)

### Co√ªts Op√©rationnels (Dev/Test)

| Service | Usage | Co√ªt/Mois | Total 1.75 mois |
|---------|-------|-----------|-----------------|
| **OpenAI Embeddings** | Tests ingestion | $50 | $88 |
| **Anthropic Claude** | Tests Q&A | $100 | $175 |
| **Infrastructure AWS** | Dev/staging | $200 | $350 |
| **GitHub Actions** | CI/CD minutes | $50 | $88 |
| **üÜï Redis** | Quota tracking | $10 | $18 |
| **üÜï Email Service** | SendGrid/SES | $15 | $26 |
| **üÜï WebSocket** | Notifications | $25 | $44 |
| **Total Ops** | | | **$789** |

**Budget Total Phase 2**: **~$42,289** (vs $37,100 initial)

**Augmentation**: +$5,189 (+14%) pour syst√®me gestion timeouts

### ROI Attendu

**Gains Phase 2**:
- Performance: -95% temps ingestion ‚Üí Support 10x plus de tenders
- Qualit√©: +15-20pts confidence ‚Üí Moins de v√©rifications manuelles
- Compl√©tude: Usage #1 KB ‚Üí Suggestions automatiques (gain 30-50% temps r√©daction)

**Estimation ROI**:
- Gain temps bid manager: 50 tenders/mois √ó 2h √©conomis√©es √ó $50/h = **$5,000/mois**
- üÜï Gain r√©duction erreurs: Moins de tenders perdus = **+$500/mois**
- ROI net: $5,500/mois - $1,100/mois (ops) = **$4,400/mois**
- **Retour sur investissement**: 10 mois (vs dev $42k)

**Justification co√ªt additionnel gestion timeouts**:
- ‚úÖ √âvite perte de donn√©es (corruption base = ‚Ç¨50k+ dommages)
- ‚úÖ Am√©liore confiance utilisateur (+20% adoption)
- ‚úÖ R√©duit support technique (-2h/semaine = $400/mois)
- ‚úÖ Permet scale (support 100+ tenders/mois vs 20 actuellement)

---

## üöß Risques & Mitigation

### Risque 1: Performance Batch Processing

**Risque**: Batch processing ne r√©duit pas suffisamment le temps

**Impact**: Moyen (objectif < 30s non atteint)

**Probabilit√©**: Faible (20%)

**Mitigation**:
- Tester batch size optimal (50, 100, 200)
- Monitoring temps par batch
- Fallback: R√©duire chunk size (1000 ‚Üí 500 tokens)

---

### Risque 2: Qualit√© Donn√©es Historical Tenders

**Risque**: Manque de tenders historiques de qualit√©

**Impact**: √âlev√© (Usage #1 KB non valid√©)

**Probabilit√©**: Moyenne (40%)

**Mitigation**:
- Identifier sources d√®s Sprint 1
- Contacter bid managers pour archives
- Fallback: Utiliser tenders publics BOAMP

---

### Risque 3: Recall@5 < 80%

**Risque**: M√©triques qualit√© en-dessous objectif

**Impact**: Moyen (acceptance utilisateur r√©duite)

**Probabilit√©**: Moyenne (30%)

**Mitigation**:
- Fine-tuning embeddings sur domaine (appels d'offres IT)
- Am√©liorer chunking strategy
- Impl√©menter re-ranking (Cohere/Voyage)
- Enrichir prompt LLM Q&A

---

### Risque 4: Coverage Tests < 80%

**Risque**: Difficult√© √† atteindre coverage

**Impact**: Faible (report Sprint suivant)

**Probabilit√©**: Moyenne (40%)

**Mitigation**:
- Prioriser tests critiques (RAG, Parser, LLM)
- Accepter 70% si tests E2E solides
- Exclure du coverage: migrations, fixtures

---

## üìö D√©pendances & Pr√©requis

### Techniques

- ‚úÖ Phase 1 compl√©t√©e et valid√©e
- ‚úÖ Infrastructure Docker op√©rationnelle
- ‚úÖ Base PostgreSQL + pgvector
- ‚úÖ APIs OpenAI + Anthropic configur√©es

### Donn√©es

- ‚è≥ 5 tenders historiques √† collecter
- ‚è≥ M√©mos techniques tenders gagn√©s
- ‚è≥ Validation bid manager pour golden dataset

### Ressources

- ‚úÖ 1 Dev Backend Python (full-time 6 semaines)
- ‚è≥ 0.5 QA Engineer (semaines 5-6)
- ‚è≥ 0.5 DevOps Engineer (semaine 6)
- ‚è≥ Expert m√©tier (validation dataset)

---

## üéØ Crit√®res d'Acceptance Phase 2

### Sprint 1: Performance ‚úÖ ou ‚ùå

- [ ] Ingestion CCTP.pdf < 10s
- [ ] Ingestion 3 documents < 30s
- [ ] 100% documents ing√©r√©s (4/4)
- [ ] 350+ embeddings cr√©√©s
- [ ] Q&A fonctionne sur CCAP.pdf et RC.pdf
- [ ] Tests automatis√©s de performance

**Condition de passage Sprint 2**: 5/6 crit√®res valid√©s

---

### Sprint 2: Knowledge Base ‚úÖ ou ‚ùå

- [ ] 5+ historical_tenders en base
- [ ] ~1500 embeddings KB cr√©√©s
- [ ] Endpoint `/suggestions` retourne r√©sultats
- [ ] Filtrage `status="won"` valid√©
- [ ] Recall@10 > 75% (tol√©rance -5pts)
- [ ] Diversit√©: ‚â• 3 tenders sources

**Condition de passage Sprint 3**: 5/6 crit√®res valid√©s

---

### Sprint 3: Qualit√© ‚úÖ ou ‚ùå

- [ ] Golden dataset (50 Q&A) cr√©√©
- [ ] Recall@5 > 75% (tol√©rance -5pts)
- [ ] MRR > 0.65 (tol√©rance -0.05)
- [ ] Test coverage > 75% (tol√©rance -5pts)
- [ ] CI/CD GitHub Actions op√©rationnel
- [ ] Tests automatiques sur PRs

**Condition de passage Phase 3**: 5/6 crit√®res valid√©s

---

## üìÖ Jalons Phase 2

| Jalon | Date | Livrables | Crit√®res Validation |
|-------|------|-----------|---------------------|
| **M1: Fin Sprint 1** | J+21 (+7j) | Batch processing, multi-docs, üÜï timeouts/retry | Temps < 30s, 100% docs, rollback OK |
| **M2: Fin Sprint 2** | J+35 | KB op√©rationnelle, endpoint /suggestions valid√© | 5+ tenders KB, Recall@10 > 75% |
| **M3: Fin Sprint 3** | J+49 | Tests complets, CI/CD, golden dataset | Coverage > 75%, CI/CD green |
| **M4: D√©mo Phase 2** | J+52 | Pr√©sentation r√©sultats, benchmark report | Validation stakeholders |

**üÜï Changement**: Jalon M1 d√©cal√© de J+14 √† J+21 pour int√©grer semaine 3 (gestion timeouts)

---

## üöÄ Prochaines √âtapes (Post Phase 2)

### Phase 3: Frontend MVP (4 semaines)

**Objectif**: Interface utilisateur pour bid managers

**Features**:
- Dashboard tenders (liste, filtres, status)
- Upload documents (drag & drop multi-fichiers)
- Visualisation analyses (graphiques, KPI, timeline)
- Chat Q&A en temps r√©el
- Suggestions KB avec preview

### Phase 4: Optimisations Avanc√©es (4 semaines)

**Objectif**: Fine-tuning et fonctionnalit√©s premium

**Features**:
- Fine-tuning embeddings domaine-specific
- Re-ranking avec Cohere/Voyage AI
- Parsing tableaux avanc√© (Camelot)
- Monitoring & alertes (Grafana, Sentry)
- G√©n√©ration m√©mo technique automatique

---

## üìù Conclusion

La Phase 2 est **critique** pour transformer le MVP Phase 1 en produit utilisable √† grande √©chelle. Focus sur:

1. **Performance** ‚Üí Ingestion 12x plus rapide
2. **Compl√©tude** ‚Üí Usage #1 KB op√©rationnel
3. **Qualit√©** ‚Üí M√©triques mesur√©es et optimis√©es
4. **Stabilit√©** ‚Üí Tests automatis√©s (80% coverage)
5. **üÜï Robustesse** ‚Üí Gestion timeouts, rollback, retry intelligent

**Dur√©e**: 7 semaines (+1 semaine vs plan initial)
**Budget**: ~$42k (+$5k pour gestion timeouts)
**ROI**: $4.4k/mois (retour 10 mois)

**üéØ Priorit√©s absolues (ne peuvent pas √™tre skipp√©es)**:
- ‚úÖ Batch processing OpenAI (critique performance)
- ‚úÖ Gestion timeouts + rollback (critique robustesse)
- ‚úÖ Notifications utilisateur (critique UX)

**Validation**: D√©mo √† bid managers pour feedback et ajustements Phase 3.

**Architecture compl√®te**: Voir [ARCHITECTURE_TIMEOUT_MANAGEMENT.md](ARCHITECTURE_TIMEOUT_MANAGEMENT.md) pour d√©tails techniques syst√®me gestion timeouts.

---

**Prochaine r√©vision**: Fin Sprint 1 (J+14)
**Responsable Phase 2**: [√Ä d√©finir]
**Stakeholders**: Bid Managers, Dev Team, Product Owner
