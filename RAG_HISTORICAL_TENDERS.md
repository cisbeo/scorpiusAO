# ğŸ“š RAG Knowledge Base - Tenders Historiques

**Date**: 2 octobre 2025
**Version**: 1.0
**Objectif**: DÃ©finir comment intÃ©grer les appels d'offres passÃ©s dans la Knowledge Base

---

## ğŸ¯ ProblÃ©matique

Actuellement, la KB stocke uniquement les **rÃ©ponses gagnantes** (`past_proposals`). Cependant, pour maximiser l'apprentissage et la pertinence des suggestions RAG, il faut Ã©galement stocker les **appels d'offres originaux** (tenders) qui ont donnÃ© lieu Ã  ces rÃ©ponses.

### Pourquoi Stocker les Tenders Historiques ?

#### 1. **Matching Contextuel AmÃ©liorÃ©**
Permet de comparer le tender actuel avec des tenders passÃ©s similaires et retrouver:
- Les rÃ©ponses qui ont fonctionnÃ© sur des AO similaires
- Les stratÃ©gies gagnantes par type de tender
- Les piÃ¨ges Ã  Ã©viter (critÃ¨res sous-estimÃ©s, exigences cachÃ©es)

#### 2. **Apprendre des Ã‰checs**
Stocker aussi les **tenders perdus** permet de:
- Identifier pourquoi une rÃ©ponse a Ã©chouÃ©
- Comparer rÃ©ponse gagnante vs rÃ©ponse perdante sur mÃªme type d'AO
- Ã‰viter de reproduire les mÃªmes erreurs

#### 3. **Analyse Comparative**
Comparer les critÃ¨res d'Ã©valuation:
- Ã‰volution des critÃ¨res dans le temps
- Poids technique vs financier par secteur
- Nouvelles exigences Ã©mergentes (cybersÃ©curitÃ©, RSE...)

#### 4. **Recherche SÃ©mantique Enrichie**
Rechercher dans les CCTP/RC passÃ©s:
- "Quels tenders demandaient ISO 27001 ?"
- "RÃ©fÃ©rences aux processus ITIL dans les AO santÃ©"
- "Ã‰volution des SLA dans les collectivitÃ©s"

---

## ğŸ—‚ï¸ SchÃ©ma de DonnÃ©es Ã‰tendu

### Nouvelle Table: `historical_tenders`

**RÃ´le**: Stocke les appels d'offres passÃ©s avec lien vers les rÃ©ponses soumises

```sql
CREATE TABLE historical_tenders (
    -- IdentitÃ©
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    kb_document_id UUID REFERENCES kb_documents(id) ON DELETE SET NULL,
    -- NULL si pas de document KB (juste mÃ©tadonnÃ©es)

    -- Informations Tender
    title VARCHAR(500) NOT NULL,
    organization VARCHAR(200) NOT NULL,
    organization_type VARCHAR(100),  -- EPT, Mairie, DÃ©partement, HÃ´pital...
    reference_number VARCHAR(100),
    publication_date DATE,
    deadline DATE NOT NULL,

    -- Secteur & Contexte
    sector VARCHAR(100),  -- CollectivitÃ©, SantÃ©, Ã‰ducation, Industrie...
    geographic_zone VARCHAR(200),  -- Ãle-de-France, PACA, National...

    -- Taille Organisation
    estimated_users INT,  -- Nombre utilisateurs finaux
    estimated_sites INT,  -- Nombre sites/implantations
    estimated_budget DECIMAL(15,2),  -- Budget estimÃ© (â‚¬)

    -- Type MarchÃ©
    contract_type VARCHAR(100),  -- InfogÃ©rance, Support, HÃ©bergement, TMA...
    contract_duration_months INT,
    contract_renewal_possible BOOLEAN,

    -- ProcÃ©dure
    procedure_type VARCHAR(50),  -- Ouvert, Restreint, Dialogue compÃ©titif...
    lot_number INT,  -- Si marchÃ© Ã  lots
    total_lots INT,

    -- Services DemandÃ©s
    services JSONB DEFAULT '[]',
    -- ["InfogÃ©rance", "Support N1-N2", "HÃ©bergement", "Supervision"]

    -- Documents Tender
    documents JSONB DEFAULT '[]',
    -- [
    --   {"type": "CCTP", "kb_document_id": "uuid", "pages": 69},
    --   {"type": "CCAP", "kb_document_id": "uuid", "pages": 38},
    --   {"type": "RC", "kb_document_id": "uuid", "pages": 12}
    -- ]

    -- CritÃ¨res Ã‰valuation
    evaluation_criteria JSONB,
    -- {
    --   "technique": {"weight": 40, "sub_criteria": [...]},
    --   "financier": {"weight": 60, "sub_criteria": [...]},
    --   "rse": {"weight": 10, "sub_criteria": [...]}
    -- }

    -- Exigences ClÃ©s
    mandatory_certifications JSONB DEFAULT '[]',
    -- ["ISO 27001", "HDS", "Qualiopi"]
    mandatory_references INT,  -- Nombre rÃ©fÃ©rences exigÃ©es
    min_team_size INT,

    -- Technologies MentionnÃ©es
    technologies JSONB DEFAULT '[]',
    -- ["ServiceNow", "VMware", "Zabbix"] - outils imposÃ©s ou suggÃ©rÃ©s

    -- Processus ITIL DemandÃ©s
    itil_processes JSONB DEFAULT '[]',
    -- ["Incident Management", "Change Management", "Problem Management"]

    -- Niveau Service (SLA)
    sla_requirements JSONB,
    -- {
    --   "availability": "99.7%",
    --   "p1_response_time": "15min",
    --   "p1_resolution_time": "4h"
    -- }

    -- RÃ©sultat Notre Participation
    participated BOOLEAN DEFAULT true,
    result VARCHAR(20) CHECK (result IN ('won', 'lost', 'abandoned', 'disqualified', 'not_submitted')),
    our_rank INT,  -- Notre classement (1 = gagnant)
    total_candidates INT,  -- Nombre total candidats
    our_score DECIMAL(5,2),  -- Notre note (/20 ou /100)
    winner_score DECIMAL(5,2),  -- Note du gagnant

    -- Lien vers Notre RÃ©ponse
    our_proposal_id UUID REFERENCES past_proposals(id) ON DELETE SET NULL,
    -- NULL si pas rÃ©pondu ou rÃ©ponse non archivÃ©e

    -- Informations Gagnant (si connu)
    winner_name VARCHAR(200),
    winner_contract_value DECIMAL(15,2),

    -- Analyse Post-Mortem
    lessons_learned TEXT,  -- Pourquoi gagnÃ©/perdu, leÃ§ons apprises
    competitive_analysis TEXT,  -- Analyse concurrence
    difficulty_level VARCHAR(20) CHECK (difficulty_level IN ('easy', 'medium', 'hard', 'very_hard')),

    -- Source
    source_platform VARCHAR(50),  -- BOAMP, AWS PLACE, Achats-publics.fr...
    source_url VARCHAR(500),

    -- MÃ©tadonnÃ©es
    tags JSONB DEFAULT '[]',
    notes TEXT,

    -- Timestamps
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    archived_at TIMESTAMP,  -- Date archivage dans KB

    CONSTRAINT historical_tenders_pkey PRIMARY KEY (id)
);

-- Indexes
CREATE INDEX idx_historical_tenders_kb_document ON historical_tenders(kb_document_id);
CREATE INDEX idx_historical_tenders_organization ON historical_tenders(organization);
CREATE INDEX idx_historical_tenders_sector ON historical_tenders(sector);
CREATE INDEX idx_historical_tenders_deadline ON historical_tenders(deadline DESC);
CREATE INDEX idx_historical_tenders_result ON historical_tenders(result);
CREATE INDEX idx_historical_tenders_participated ON historical_tenders(participated);
CREATE INDEX idx_historical_tenders_our_proposal ON historical_tenders(our_proposal_id);
CREATE INDEX idx_historical_tenders_services ON historical_tenders USING GIN (services);
CREATE INDEX idx_historical_tenders_technologies ON historical_tenders USING GIN (technologies);
CREATE INDEX idx_historical_tenders_certifications ON historical_tenders USING GIN (mandatory_certifications);
CREATE INDEX idx_historical_tenders_tags ON historical_tenders USING GIN (tags);

-- Index composite pour recherche similaire
CREATE INDEX idx_historical_tenders_similarity ON historical_tenders(
    sector, contract_type, estimated_users, result
);
```

---

### Table ModifiÃ©e: `past_proposals` (avec lien tender)

Ajouter colonne pour lier Ã  `historical_tenders`:

```sql
ALTER TABLE past_proposals
ADD COLUMN historical_tender_id UUID REFERENCES historical_tenders(id) ON DELETE SET NULL;

-- Index
CREATE INDEX idx_past_proposals_historical_tender ON past_proposals(historical_tender_id);
```

**BidirectionnalitÃ©**:
- `historical_tenders.our_proposal_id` â†’ `past_proposals.id`
- `past_proposals.historical_tender_id` â†’ `historical_tenders.id`

---

## ğŸ¯ Nouveaux Use Cases RAG

### Use Case #6: Analyse Comparative Tenders Similaires â­â­â­â­

**ProblÃ¨me**: Bid manager ne sait pas comment ce tender se compare aux AO passÃ©s

**Solution RAG**:

```
1. Nouveau tender reÃ§u â†’ Extraction contexte automatique:
   - Secteur: CollectivitÃ©
   - Taille: 1200 users
   - Services: InfogÃ©rance + Support N1
   - Budget estimÃ©: 600kâ‚¬/an

2. RAG recherche dans historical_tenders:
   - Matching similaritÃ© sÃ©mantique (embedding CCTP)
   - Filtres mÃ©tadonnÃ©es (secteur, taille Â±30%, services similaires)
   - Retourne Top 5 tenders les plus similaires

3. Pour chaque tender similaire:
   - Affiche critÃ¨res d'Ã©valuation comparÃ©s
   - Montre notre rÃ©sultat (gagnÃ©/perdu + score)
   - SuggÃ¨re stratÃ©gie gagnante si applicable

4. Dashboard comparatif:
   - "3 tenders similaires gagnÃ©s dans les 2 ans"
   - "CritÃ¨res techniques poids moyen: 42% (vs 40% ici)"
   - "SLA P1 typique: <15min (vs <20min demandÃ© ici)"
```

**UI Mockup**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š Tenders Similaires PassÃ©s                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ 5 tenders similaires trouvÃ©s (2020-2024)                   â”‚
â”‚                                                             â”‚
â”‚ âœ… 1. EPT Val-de-Marne - InfogÃ©rance (2020)   Score: 95%  â”‚
â”‚     Secteur: CollectivitÃ© âœ“  |  1500 users âœ“  |  WON ğŸ†   â”‚
â”‚     Notre score: 18.2/20 (1er/4)                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ CritÃ¨res clÃ©s:                                     â”‚ â”‚
â”‚     â”‚ â€¢ Technique: 40% (vs 40% actuel) âœ“                â”‚ â”‚
â”‚     â”‚ â€¢ Financier: 60% (vs 60% actuel) âœ“                â”‚ â”‚
â”‚     â”‚ â€¢ ISO 27001 obligatoire (vs obligatoire) âœ“        â”‚ â”‚
â”‚     â”‚ â€¢ SLA P1: <15min (vs <20min actuel) âš ï¸ Plus strictâ”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚     ğŸ’¡ StratÃ©gie gagnante:                                 â”‚
â”‚        - Mise en avant ITIL v4 (diffÃ©renciateur clÃ©)      â”‚
â”‚        - Ã‰quipe 100% dÃ©diÃ©e (exigence implicite)          â”‚
â”‚        - Prix agressif lots 1+2 combinÃ©s                  â”‚
â”‚     [Voir rÃ©ponse gagnante] [Voir CCTP original]          â”‚
â”‚                                                             â”‚
â”‚ âŒ 2. Mairie Montreuil - Support IT (2022)    Score: 87%  â”‚
â”‚     Secteur: CollectivitÃ© âœ“  |  800 users âš ï¸  |  LOST âŒ  â”‚
â”‚     Notre score: 16.5/20 (2Ã¨me/5)                         â”‚
â”‚     Gagnant: 17.8/20 (Ã©cart: -1.3 pts)                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ CritÃ¨res clÃ©s:                                     â”‚ â”‚
â”‚     â”‚ â€¢ Technique: 30% (vs 40% actuel) âš ï¸ Moins importantâ”‚ â”‚
â”‚     â”‚ â€¢ Financier: 70% (vs 60% actuel) âš ï¸ Prix critique â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚     âš ï¸  LeÃ§ons apprises:                                   â”‚
â”‚        - Prix trop Ã©levÃ© (-5 pts financier)               â”‚
â”‚        - Manque rÃ©fÃ©rences collectivitÃ©s <1000 users      â”‚
â”‚        - MÃ©thodologie trop complexe pour petit marchÃ©     â”‚
â”‚     [Voir rÃ©ponse soumise] [Voir analyse post-mortem]     â”‚
â”‚                                                             â”‚
â”‚ âœ… 3. CD Seine-et-Marne - InfogÃ©rance (2023)  Score: 82%  â”‚
â”‚     [...]                                                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“ˆ Analyse Comparative                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ Taux succÃ¨s sur tenders similaires: 60% (3/5) âœ…          â”‚
â”‚                                                             â”‚
â”‚ CritÃ¨res moyens tenders similaires:                        â”‚
â”‚ â€¢ Technique: 38% (actuel: 40%) â†’ LÃ©gÃ¨rement + important   â”‚
â”‚ â€¢ Financier: 62% (actuel: 60%) â†’ Standard                 â”‚
â”‚                                                             â”‚
â”‚ SLA typiques:                                              â”‚
â”‚ â€¢ P1 response: <12min mÃ©diane (actuel: <20min) âš ï¸         â”‚
â”‚ â€¢ P1 resolution: 3h mÃ©diane (actuel: 4h) âœ…               â”‚
â”‚                                                             â”‚
â”‚ Certifications demandÃ©es (frÃ©quence):                      â”‚
â”‚ â€¢ ISO 27001: 100% (5/5) â†’ CRITIQUE                        â”‚
â”‚ â€¢ ISO 9001: 60% (3/5) â†’ Important                         â”‚
â”‚ â€¢ HDS: 20% (1/5) â†’ Optionnel                              â”‚
â”‚                                                             â”‚
â”‚ ğŸ’¡ Recommandations stratÃ©giques:                           â”‚
â”‚ 1. Mettre en avant notre taux succÃ¨s 60% sur AO similairesâ”‚
â”‚ 2. Anticiper nÃ©gociation SLA P1 (<20min peut Ãªtre nÃ©gociÃ©)â”‚
â”‚ 3. Prix compÃ©titif critique (â‰¤650kâ‚¬/an recommandÃ©)        â”‚
â”‚ 4. Insister sur rÃ©fÃ©rences EPT (diffÃ©renciateur clÃ©)      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Gain**: **2-3h** par tender (recherche manuelle + analyse compÃ©titive)
**ROI**: **TrÃ¨s Ã©levÃ©** (rÃ©duit risque Ã©chec)

---

### Use Case #7: Apprentissage des Ã‰checs â­â­â­â­

**ProblÃ¨me**: On reproduit les mÃªmes erreurs sur des tenders similaires

**Solution RAG**:

```
1. DÃ©tection tender Ã  risque:
   RAG identifie tenders perdus similaires (secteur + taille + services)

2. Analyse des raisons d'Ã©chec:
   - Extraction "lessons_learned" des tenders perdus
   - Identification patterns d'Ã©chec rÃ©currents
   - Alertes proactives bid manager

3. Checklist personnalisÃ©e:
   GÃ©nÃ©ration checklist basÃ©e sur Ã©checs passÃ©s:
   âœ“ VÃ©rifier prix â‰¤ budget marchÃ© (Ã©chec Montreuil 2022: prix trop Ã©levÃ©)
   âœ“ Minimum 3 rÃ©fÃ©rences secteur identique (Ã©chec Lyon 2021)
   âœ“ Ã‰quipe dÃ©diÃ©e mentionnÃ©e (Ã©chec APHP 2023)
```

**Exemple Alerte**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸  ALERTE: Risques IdentifiÃ©s (basÃ©s sur historique)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ ğŸ” 3 tenders similaires perdus identifiÃ©s:                 â”‚
â”‚                                                             â”‚
â”‚ 1. Mairie Montreuil (2022) - Perdu 2Ã¨me/5                 â”‚
â”‚    Raison: Prix 18% au-dessus gagnant                     â”‚
â”‚    âš ï¸  RISQUE: Budget actuel semble serrÃ© (600kâ‚¬)         â”‚
â”‚    ğŸ’¡ ACTION: Proposer 2 variantes (base + options)       â”‚
â”‚                                                             â”‚
â”‚ 2. RÃ©gion PACA (2021) - Perdu 3Ã¨me/6                      â”‚
â”‚    Raison: Manque rÃ©fÃ©rences collectivitÃ©s >1000 users    â”‚
â”‚    âš ï¸  RISQUE: Seulement 1 rÃ©fÃ©rence >1000 users en KB    â”‚
â”‚    ğŸ’¡ ACTION: Mettre en avant CD77 (2200 users)           â”‚
â”‚                                                             â”‚
â”‚ 3. APHP (2023) - Perdu 4Ã¨me/7                             â”‚
â”‚    Raison: Ã‰quipe non dÃ©diÃ©e explicitement mentionnÃ©e     â”‚
â”‚    âš ï¸  RISQUE: CCTP demande "Ã©quipe dÃ©diÃ©e" (art. 3.2)    â”‚
â”‚    ğŸ’¡ ACTION: Insister section "Moyens humains" sur dÃ©diÃ© â”‚
â”‚                                                             â”‚
â”‚ [Voir analyses dÃ©taillÃ©es] [GÃ©nÃ©rer plan mitigation]      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Gain**: **1-2h** + **RÃ©duction risque Ã©chec 20-30%**
**ROI**: **TrÃ¨s Ã©levÃ©** (Ã©vite soumissions vouÃ©es Ã  l'Ã©chec)

---

## ğŸ”„ Workflow d'Archivage

### Processus AutomatisÃ©

```
1. Tender actif dans table `tenders` (systÃ¨me existant)
   â†“
2. RÃ©ponse soumise â†’ CrÃ©ation `proposals`
   â†“
3. RÃ©sultat connu (gagnÃ©/perdu)
   â†“
4. Archivage automatique dans KB:

   a) CrÃ©er historical_tender:
      - Copier mÃ©tadonnÃ©es tender
      - Extraire critÃ¨res, exigences, SLA
      - Stocker rÃ©sultat (won/lost + score)

   b) Lier avec past_proposal:
      - Si gagnÃ©: marquer "won = true"
      - Si perdu: marquer "won = false" + raisons Ã©chec

   c) CrÃ©er embeddings documents tender:
      - CCTP â†’ chunks â†’ embeddings
      - CCAP â†’ chunks â†’ embeddings
      - RC â†’ chunks â†’ embeddings

   d) Enrichir mÃ©tadonnÃ©es:
      - Tags automatiques (secteur, services, technologies)
      - Extraction lessons_learned (formulaire post-mortem)
      - Analyse comparative vs gagnant (si infos disponibles)
```

### Trigger Automatique

```sql
-- Fonction archivage automatique
CREATE OR REPLACE FUNCTION archive_tender_to_kb()
RETURNS TRIGGER AS $$
BEGIN
    -- Si statut passe Ã  'completed' ou 'closed'
    IF NEW.status IN ('completed', 'closed') AND OLD.status NOT IN ('completed', 'closed') THEN
        -- Insertion dans historical_tenders
        INSERT INTO historical_tenders (
            title, organization, reference_number, deadline,
            sector, services, result, participated,
            -- ... autres champs
        )
        SELECT
            t.title, t.organization, t.reference_number, t.deadline,
            t.parsed_content->>'sector',
            t.parsed_content->'services',
            -- DÃ©terminer result depuis proposal
            CASE
                WHEN EXISTS (SELECT 1 FROM proposals p WHERE p.tender_id = t.id AND p.status = 'won') THEN 'won'
                WHEN EXISTS (SELECT 1 FROM proposals p WHERE p.tender_id = t.id AND p.status = 'lost') THEN 'lost'
                ELSE 'not_submitted'
            END,
            true  -- participated
        FROM tenders t
        WHERE t.id = NEW.id;

        -- Trigger crÃ©ation embeddings asynchrone (Celery task)
        -- PERFORM pg_notify('archive_tender', NEW.id::text);
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Trigger
CREATE TRIGGER trigger_archive_tender_to_kb
AFTER UPDATE ON tenders
FOR EACH ROW
EXECUTE FUNCTION archive_tender_to_kb();
```

---

## ğŸ“Š Statistiques & Analytics

### Vue: Taux SuccÃ¨s par Type Tender

```sql
CREATE VIEW v_success_rate_by_tender_type AS
SELECT
    sector,
    contract_type,
    COUNT(*) as total_tenders,
    SUM(CASE WHEN result = 'won' THEN 1 ELSE 0 END) as won_count,
    ROUND(100.0 * SUM(CASE WHEN result = 'won' THEN 1 ELSE 0 END) / COUNT(*), 2) as win_rate,
    AVG(CASE WHEN result = 'won' THEN our_score END) as avg_score_won,
    AVG(CASE WHEN result = 'lost' THEN our_score END) as avg_score_lost,
    AVG(CASE WHEN result = 'won' THEN our_rank END) as avg_rank_won
FROM historical_tenders
WHERE participated = true
  AND result IN ('won', 'lost')
GROUP BY sector, contract_type
ORDER BY win_rate DESC;
```

### Vue: Ã‰volution CritÃ¨res dans le Temps

```sql
CREATE VIEW v_evaluation_criteria_evolution AS
SELECT
    DATE_TRUNC('year', deadline) as year,
    sector,
    AVG((evaluation_criteria->'technique'->>'weight')::float) as avg_tech_weight,
    AVG((evaluation_criteria->'financier'->>'weight')::float) as avg_financial_weight,
    AVG((evaluation_criteria->'rse'->>'weight')::float) as avg_rse_weight,
    COUNT(*) as tender_count
FROM historical_tenders
WHERE evaluation_criteria IS NOT NULL
GROUP BY DATE_TRUNC('year', deadline), sector
ORDER BY year DESC, sector;
```

### RequÃªte: Top Raisons Ã‰checs

```sql
SELECT
    sector,
    COUNT(*) as lost_count,
    -- Extraction patterns textuels des lessons_learned
    jsonb_agg(DISTINCT jsonb_build_object(
        'tender', title,
        'lesson', lessons_learned
    )) as common_failure_reasons
FROM historical_tenders
WHERE result = 'lost'
  AND lessons_learned IS NOT NULL
GROUP BY sector
ORDER BY lost_count DESC;
```

---

## ğŸ¯ MÃ©triques de SuccÃ¨s

### KPIs Nouveaux

| KPI | Objectif | Mesure |
|-----|----------|--------|
| **Taux archivage** | 100% | % tenders complÃ©tÃ©s archivÃ©s dans KB |
| **ComplÃ©tude post-mortem** | >80% | % tenders avec lessons_learned remplis |
| **Utilisation comparaisons** | >70% | % nouveaux tenders avec analyse comparative consultÃ©e |
| **RÃ©duction Ã©checs rÃ©pÃ©tÃ©s** | -30% | RÃ©duction tenders perdus pour mÃªmes raisons |
| **Taux succÃ¨s global** | +10% | AmÃ©lioration win rate aprÃ¨s 6 mois |

---

## ğŸš€ Plan ImplÃ©mentation

### Phase 1: Infrastructure (Semaine 1)
1. âœ… CrÃ©er table `historical_tenders`
2. âœ… Modifier table `past_proposals` (ajout colonne)
3. âœ… CrÃ©er vues analytics
4. âœ… DÃ©velopper trigger archivage automatique

### Phase 2: Embeddings Tenders (Semaine 2)
1. âœ… Adapter RAG service pour embeddings tenders
2. âœ… Task Celery archivage avec embeddings
3. âœ… Tests sur 5-10 tenders historiques

### Phase 3: Use Cases (Semaine 3)
1. âœ… Use Case #6: Analyse comparative
2. âœ… Use Case #7: Apprentissage Ã©checs
3. âœ… UI alertes et recommandations

### Phase 4: Analytics (Semaine 4)
1. âœ… Dashboard taux succÃ¨s par secteur
2. âœ… Ã‰volution critÃ¨res dans le temps
3. âœ… Rapports post-mortem automatiques

---

## ğŸ“‹ Checklist Migration DonnÃ©es Historiques

Pour migrer tenders existants dans KB:

```python
# Script migration backend/scripts/migrate_historical_tenders.py

async def migrate_existing_tenders():
    """Migrer tenders existants (status = completed/closed) vers KB"""

    # 1. RÃ©cupÃ©rer tenders complÃ©tÃ©s
    tenders = await db.execute(
        select(Tender).where(Tender.status.in_(['completed', 'closed']))
    )

    for tender in tenders:
        # 2. CrÃ©er historical_tender
        historical = HistoricalTender(
            title=tender.title,
            organization=tender.organization,
            reference_number=tender.reference_number,
            deadline=tender.deadline,
            # ... extraire mÃ©tadonnÃ©es
        )

        # 3. Lier avec proposal si existe
        proposal = await db.execute(
            select(Proposal).where(Proposal.tender_id == tender.id)
        )
        if proposal:
            historical.our_proposal_id = proposal.id
            historical.result = 'won' if proposal.status == 'won' else 'lost'

        # 4. CrÃ©er embeddings documents
        for doc in tender.documents:
            await rag_service.ingest_document(
                document_id=doc.id,
                content=doc.extracted_text,
                document_type='historical_tender',
                metadata={'tender_id': historical.id}
            )

        await db.commit()
```

---

## ğŸ’¡ Recommandations

### 1. Formulaire Post-Mortem Obligatoire

AprÃ¨s chaque tender (gagnÃ© ou perdu), formulaire structurÃ©:

```
ğŸ“ Analyse Post-Mortem Tender: {title}

RÃ©sultat: [x] GagnÃ©  [ ] Perdu  [ ] AbandonnÃ©

Notre score: __/20    Rang: __/__    Gagnant score: __/20

Pourquoi gagnÃ©/perdu? (3-5 lignes)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Facteurs clÃ©s succÃ¨s/Ã©chec:
â˜ Prix (trop Ã©levÃ©/compÃ©titif)
â˜ RÃ©fÃ©rences (suffisantes/insuffisantes)
â˜ Ã‰quipe (dÃ©diÃ©e/mutualisÃ©e)
â˜ MÃ©thodologie (adaptÃ©e/trop complexe)
â˜ Certifications (toutes prÃ©sentes/manquantes)
â˜ DÃ©lais (respectÃ©s/trop justes)
â˜ Autre: ________________

LeÃ§ons pour prochains tenders similaires:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Revue Trimestrielle Tenders Perdus

Analyser patterns:
- Tenders perdus par secteur
- Raisons rÃ©currentes d'Ã©chec
- Actions correctives Ã  mettre en place

### 3. Benchmark Concurrentiel

Stocker infos gagnants quand disponibles:
- Nom concurrent gagnant
- Prix gagnant (si publiÃ©)
- Points forts identifiÃ©s
- â†’ Constituer base connaissance concurrence

---

**Prochaine Ã©tape**: CrÃ©er migration Alembic pour `historical_tenders`

**DerniÃ¨re mise Ã  jour**: 2 octobre 2025
**Version**: 1.0
